{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transfer Learning\n",
    "\n",
    "For low end devices it will be hard to manage all the resources required to train the neural network (Labeling Data, Computational Resources and Time). To solve this problem we've transfer learning. \n",
    "\n",
    "With Transfer Learning we can use a pre-trained model and then fine tune it for another related task. \n",
    "\n",
    "For Image Classification task they are already trained on the `ImageNet` dataset.\n",
    "\n",
    "#### **Types of Transfer Learning**\n",
    "\n",
    "**Feature Extraction** : Leverages the pre-trained model features. We remove the last few layers of the pre-trained model and add a new classification layer. Then we only train the newly added classification layer on the dataset. So the weights and biases remains `frozen` i.e. not updated during backpropagation. Only the weights of the added layer will change during backpropagation. It requires less data and resources. \n",
    "\n",
    "**Fine Tuning** : Adjusting pre-trained model's weights and biases for new task. We can allow the weights of the pre-trained model to update during backpropagation. Also we can add new classification layers as well. It requires more data and resources.\n",
    "\n",
    "#### **Best Practice for Transfer Learning**\n",
    "\n",
    "[Best Practice for Transfer Learning](https://www.linkedin.com/learning/computer-vision-for-data-scientists/best-practices-for-transfer-learning?autoSkip=true&resume=false&u=42288921)\n",
    "\n",
    "#### PyTorch\n",
    "\n",
    "It is a popular library for creating and training deep learning libraries."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
