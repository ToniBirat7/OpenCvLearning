{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evolution of CNN Architectures\n",
    "\n",
    "#### **LeNet** \n",
    "\n",
    "It was developed for Handwritten Digit Recognition (MNIST Dataset)\n",
    "\n",
    "**Architecture**\n",
    "\n",
    "`Input`: 32x32 grayscale image.\n",
    "\n",
    "`Layers`:\n",
    " - Three convolutional layers followed by average pooling. Used for feature extraction. \n",
    "   - Layer 1 uses six `(5,5)` kernels and stride of 1 i.e. `Output Shape = (32-5+)*(32-5+1)*6 = (28*28*6)`. In the first pooling layer, it converts the `(28*28*6)` feature map to `(14*14*6)` through `2*2` average pooling with stride of `2`, reduces the spatial dimensions (height and width) via average pooling. Then the output is passed to `Hyperbolic Tangent` for the activation function.\n",
    "  \n",
    "   - Layer 2 uses sixteen `(5,5)` kernels and stride of 1 i.e. `Output Shape = (14-5+1)*(14-5+1)*16 = (10*10*16)`. In the second pooling layer, it converts the `(10*10*16)` feature map to `(5*5*16)` through `2*2` average pooling with stride of `2`, reduces the spatial dimensions (height and width) via average pooling. Then the output is passed to `Hyperbolic Tangent` for the activation function. \n",
    "\n",
    "   - Layer 3 (Flatten) uses `120` `(5,5)` kernels and stride of 1 i.e. same as size of the previous feature map the output results in the `Output = (5-5+1,5-5+1,120) = (1,1,120)` feature map which are passed to a fully connected layer. The result is flattened into a `120*1` vector.\n",
    "\n",
    "<img src='../Notes_Images/DL8.png'>\n",
    "\n",
    " - Fully connected layers at the end. Used for classification. Here, it transforms `120-element vector` (Input Layer) to `84-element vector` (Hidden/Dense Layer) and Output Layer matches the number of classes i.e. `10 Neurons`. These `10 Neurons` generate the probability distribution for the classes. \n",
    "\n",
    "`Advantages`:\n",
    "\n",
    " - Reduced the need for manual feature extraction. E.g. using OpenCv libraries to extract features.\n",
    "\n",
    " - Demonstrated the effectiveness of CNNs for image recognition.\n",
    "\n",
    "<style>\n",
    ".bg{\n",
    "\tbackground-color: white;\n",
    "}\n",
    ".mar{\n",
    "\ttext-align: center;\n",
    "}\n",
    "</style>\n",
    "\n",
    "<figure>\n",
    "<img src='../Notes_Images/DL9.svg' class=\"bg\">\n",
    "<figcaption class=\"mar\">LeNet Architecture</figcaption>\n",
    "</figure>\n",
    "\n",
    "#### **AlexNet** \n",
    "\n",
    "The start of the Deep Learning Revolution. Developed by Geoffrey Hinton in 2012. They started the Deep Learning Revolution by establishing a Deeper 8 layer and wider network. Introduced the `ReLu` activation function. It was also the first Deep Learning Architecture that leveraged GPU. \n",
    "\n",
    "`AlexNet` also introduced the data augmentation techniques such as Cropping, Flipping and Color alterations to increase the size of the training dataset. It uses `Dropout` as a `Regularization` for better `Generalization`. It also introduced the concept of stacking Convolutional Layers. Stack of Convolutional Layer helps model learn more complex features. \n",
    "\n",
    "**Links for the Working Architecture of AlexNet**\n",
    "\n",
    "[Medium Article](https://medium.com/@siddheshb008/alexnet-architecture-explained-b6240c528bd5)\n",
    "\n",
    "[Second Article](https://learnopencv.com/understanding-alexnet/)\n",
    "\n",
    "[AlexNet Visualization](https://tensorspace.org/html/playground/alexnet.html)\n",
    "\n",
    "#### Working of Activation Functions in Depth\n",
    "\n",
    "Here, we'll explore the working fo Activation Function under the hood. Why it is used? How it helps to introduce non-linearity in the network? Why do we need them? How does the activation work in `Convolutional Layer` and `Fully Connected Layer`? Why do we need to down sample the image using filters? What are the computational limitations related to Neural Network? How Neural Network represented in the `GPU RAM`?\n",
    "\n",
    "#### How are the images or dataset is trained in the Neural Network? \n",
    "\n",
    "Here, we'll understand how the dataset is splited into batches, why do we need batches? How is one batch of the dataset trained parallely using `GPU`, for this tensor is used. What is tensor? How does it work? How is it represented in the GPU? \n",
    "\n",
    "**Resources**\n",
    "\n",
    "[Understanding Tensor](https://www.youtube.com/watch?v=f5liqUk0ZTw&t=735s)\n",
    "\n",
    "\n",
    "#### **VGG (Visual Geometry Group)** \n",
    "\n",
    "AlexNet is considered as the grandfather of the modern Convolutional Neural Network, `VGG` is considered the father of modern CNN Architecture Design. In 2014 it took first place at the `ImageNet Large Scale Visual Recognition` (ILSVR) challenge.\n",
    "\n",
    "Before VGG, most Neural Network used a variety of layer sizes and types making them hard to scale and generalize but `VGG` uses small `3*3` kernels which allows deep networks to work without computational load. Also, VGG comprises of very large network (up to 16 - 19) layers in comparison to AlexNet i.e. 8 layers.  \n",
    "\n",
    "It is due to this deep architecture which paved the way for the development of architectures like `ResNet` and `DenseNet`.\n",
    "\n",
    "Also, VGG models are widely used for transfer learning tasks, given their pre-trained weights on ImageNet.\n",
    "\n",
    "There are two versions of VGG i.e. `VGG16` (16 Layer) and `VGG19` (19 Layer)\n",
    " \n",
    "**Resources**\n",
    "\n",
    "[Understand VGG Architecture](https://medium.com/@siddheshb008/vgg-net-architecture-explained-71179310050f)\n",
    "\n",
    "\n",
    "#### ResNet (Residual Network)\n",
    "\n",
    "ResNet (Residual Network) is a CNN architecture introduced in 2015 by Kaiming He et al. in their paper \"Deep Residual Learning for Image Recognition.\" It is a groundbreaking architecture designed to address the **vanishing gradient problem**, which occurs in deep networks.\n",
    "\n",
    "**What is Vanishing Gradient Problem**\n",
    "\n",
    "The vanishing gradient problem arises during the training of deep neural networks, where gradients (used to update the network's weights) become exceedingly small as they are backpropagated through many layers. This causes the earlier layers of the network (closer to the input) to learn very slowly or stop learning altogether, leading to poor overall performance.\n",
    "\n",
    "<img src='../Notes_Images/DL10.png'>\n",
    "\n",
    "<img src='../Notes_Images/DL11.png'>\n",
    "\n",
    "<img src='../Notes_Images/DL12.png'>\n",
    "\n",
    "The above problem is solved using `Skip Connection`\n",
    "\n",
    "<img src='../Notes_Images/DL13.png'>\n",
    "\n",
    "**Key Idea Behind ResNet**\n",
    "\n",
    "`ResNet` introduces the concept of `residual learning` by using \"skip connections\" or \"shortcuts.\"\n",
    "\n",
    "[Skip Connection Explanation](https://theaisummer.com/skip-connections/#:~:text=ResNet%3A%20skip%20connections%20via%20addition&text=Then%20the%20gradient%20would%20simply,function%20to%20preserve%20the%20gradient.)\n",
    "\n",
    "[Back Propagation Article1](https://medium.com/@juanc.olamendy/backpropagation-in-deep-learning-the-key-to-optimizing-neural-networks-7c063a03f677)\n",
    "\n",
    "[Back Propagation Article2](https://builtin.com/machine-learning/backpropagation-neural-network)\n",
    "\n",
    "**Versiona**\n",
    "\n",
    "`ResNet50`\n",
    "`ResNet101`\n",
    "`ResNet1000`\n",
    "\n",
    "### MobileNetV1\n",
    "\n",
    "It was created to tackle the problem of implementing and running deep neural networks on mobile and embedded devices. The `MobileNet` family has 3 different architectures i.e. `V1`, `V2` and `V3`.\n",
    "\n",
    "**V1** : It utilizes depthwise separable convolutions. In this architecture the traditional `Convolutional` layer is splited into two i.e. `depthwise` and `pointwise`. It requires less computational resources.\n",
    "\n",
    "**V3** : It is the best architecture for IoT and Embedded devices.\n",
    "\n",
    "### EfficientNet\n",
    "\n",
    "It is another architecture for Low End Devices. It also performs well for small devices. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
