{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convolution Neural Network\n",
    "\n",
    "#### CNN Components\n",
    "\n",
    "**Input Layer**\n",
    "\n",
    "**Convolution Layer**\n",
    "\n",
    "**Activation Functions**\n",
    "\n",
    "**Pooling Layer**\n",
    "\n",
    "**Fully Connected Layer**\n",
    "\n",
    "**Output Layer** : Softmax is used in the `neurons` of the output layer. \n",
    "\n",
    "\n",
    "#### Working of Convolution Layer\n",
    "\n",
    "[Best Link](https://machinelearningmastery.com/convolutional-layers-for-deep-learning-neural-networks/)\n",
    "\n",
    "#### Understand about Tensor Working of Tensor\n",
    "\n",
    "[Understand Tensor](https://www.youtube.com/watch?v=f5liqUk0ZTw&t=735s)\n",
    "\n",
    "\n",
    "#### Loss Function\n",
    "\n",
    "**Cross-entropy loss (log loss)** : It is commonly used Loss Function for Image Classification\n",
    "\n",
    "\n",
    "#### Back Propagation in CNN\n",
    "\n",
    "Once the loss is calculated, we need to change the weights and biases in the Network. For that we need to know which weights `contributed` most to the loss. For this we calculate the gradient of the `loss` with respect to every parameter i.e. Derivative of Loss w.r.t `weights` and `biases`. After calculating the gradient we can know which `weight` and `bias` heavily impacted for the `loss` then we can target those `weights` and `bias` directly to improve the models performance. \n",
    "\n",
    "**Back Propagation in Convolution Layer**\n",
    "\n",
    "Back Propagation is not only limited to the start of the `Fully Connected Layer` but it goes to the `Convolution` layer. Back Propagation for the Convolution layer is calculated differently. \n",
    "\n",
    "In the `Convolution` layer the gradient is calculated w.r.t both the `input`,`feature map` and `convolution kernel`. \n",
    "\n",
    "**Back Propagation in Pooling Layer**\n",
    "\n",
    "\n",
    "#### Optimization Techniques\n",
    "\n",
    "**Gradient Descent**\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
