{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convolution Neural Network\n",
    "\n",
    "#### CNN Components\n",
    "\n",
    "**Input Layer**\n",
    "\n",
    "**Convolution Layer**\n",
    "\n",
    "**Activation Functions**\n",
    "\n",
    "**Pooling Layer**\n",
    "\n",
    "**Fully Connected Layer**\n",
    "\n",
    "**Output Layer** : Softmax is used in the `neurons` of the output layer. \n",
    "\n",
    "\n",
    "#### Working of Convolution Layer\n",
    "\n",
    "[Best Link](https://machinelearningmastery.com/convolutional-layers-for-deep-learning-neural-networks/)\n",
    "\n",
    "#### Understand about Tensor Working of Tensor\n",
    "\n",
    "[Understand Tensor](https://www.youtube.com/watch?v=f5liqUk0ZTw&t=735s)\n",
    "\n",
    "\n",
    "#### Loss Function\n",
    "\n",
    "**Cross-entropy loss (log loss)** : It is commonly used Loss Function for Image Classification\n",
    "\n",
    "\n",
    "#### Back Propagation in CNN\n",
    "\n",
    "*Note Before You Read* : Use `20 = 2x + 2y` as an reference example to understand better.\n",
    "\n",
    "Once the loss is calculated, we need to change the weights and biases in the Network. For that we need to know which weights `contributed` most to the loss. For this we calculate the gradient of the `loss` with respect to every parameter i.e. Derivative of Loss w.r.t `weights` and `biases`. After calculating the gradient we can know which `weight` and `bias` heavily impacted for the `loss` then we can target those `weights` and `bias` directly to improve the models performance. \n",
    "\n",
    "**Back Propagation in Convolution Layer**\n",
    "\n",
    "Back Propagation is not only limited to the start of the `Fully Connected Layer` but it goes to the `Convolution` layer. Back Propagation for the Convolution layer is calculated differently. \n",
    "\n",
    "In the `Convolution` layer the gradient is calculated w.r.t both the `input`,`feature map` and `convolution kernel`. \n",
    "\n",
    "**Back Propagation in Pooling Layer**\n",
    "\n",
    "\n",
    "#### Optimization Techniques\n",
    "\n",
    "**Gradient Descent** : Minimize the loss function. \n",
    "\n",
    "There are many variants of Gradient Descent Such as\n",
    "\n",
    "*Batch Gradient Descent* : Takes the entire dataset to compute the gradient and then update the model parameters. Computationally expensive and slow. Gradient is calculated as the average gradient of the loss function. \n",
    "\n",
    "*Stochastic Gradient Descent (SGD)* : Take a single data point to compute the gradient. Single training example. It is fast and accurate. It can introduce noise. \n",
    "\n",
    "*Mini-Batch Gradient Descent* : Takes mini-batch from a batch to calculate the gradient. It is efficient than `SGD`. It reduces noise. \n",
    "\n",
    "**Adam Optimization** : Adaptive learning rate method. It makes use of two adaptive learning rate method i.e. `AdaGrad` and `RMSProp`. `Adam` adjusts the learning rate for each parameter based on the moving average of the gradient's first and second moments. It makes the optimization more stable and efficient. \n",
    "\n",
    "#### Regularization and Data Augmentation\n",
    "\n",
    "Very important factors while training the Neural Network. Over fitting is a condition when the model works best on the training data but not on the other testing data which means the model is not generalized to the new testing data. There are many `Regularization` and `Data Augmentation` techniques to address this issue. Before that what is `Regularization`?\n",
    "\n",
    "**Regularization** : Regularization methods add a penalty term to the loss function, encouraging the model to learn simpler and more general patterns in the data. Once, we add a regularization term to the loss function, it discourages the model from overfitting.\n",
    "\n",
    "*Why Regularization is Needed?*\n",
    "\n",
    "When a model is too complex, it tends to learn the training data too well, including the noise and outliers. This leads to poor generalization on unseen data. Regularization balances the trade-off between the model's complexity and its performance on new data, ensuring better generalization.\n",
    "\n",
    "<img src='../Notes_Images/DL1.png'>\n",
    "\n",
    "<img src='../Notes_Images/DL2.png'>\n",
    "\n",
    "*Types of Regularization*\n",
    "\n",
    "<img src='../Notes_Images/DL3.png'>\n",
    "\n",
    "<img src='../Notes_Images/DL4.png'>\n",
    "\n",
    "**Data Augmentation** : `Data Augmentation` is a technique used to artificially expand the size of a training dataset by applying various transformations to the original data. These transformations preserve the labels of the data while introducing diversity, making the model robust to variations in the input data. In the context of `Convolutional Neural Networks` (CNNs), data augmentation is particularly useful for image data, where transformations like rotation, flipping, scaling, cropping, and color adjustments simulate real-world variations.\n",
    "\n",
    "<img src='../Notes_Images/DL5.png'>\n",
    "\n",
    "<img src='../Notes_Images/DL6.png'>\n",
    "\n",
    "<img src='../Notes_Images/DL7.png'>\n",
    "\n",
    "```python\n",
    "\tfrom tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "\t# Define augmentation parameters\n",
    "\tdatagen = ImageDataGenerator(\n",
    "\t\t\trotation_range=15,\n",
    "\t\t\twidth_shift_range=0.1,\n",
    "\t\t\theight_shift_range=0.1,\n",
    "\t\t\tzoom_range=0.2\n",
    "\t)\n",
    "\n",
    "\t# Apply augmentation to training data\n",
    "\tdatagen.fit(x_train)\n",
    "```\n",
    "\n",
    "These techniques improve the model performance on unseen data. "
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
